<!DOCTYPE html>
<html>
<head>
    <title>Pooja Gaurav</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="blog.css">
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body>
    <section id="section1">
        <div class="content-box">
            <div class="userbox1"> 
                <nav class="fixed">
                    <center class="dp">
                        <img src="./img/dp1.jpg" class="home-img"/>
                        <h2>Pooja Gaurav</h2>
                    </center> 
                    <ul class="menu">
                        <li class = "item"><a href="index.html#section1">Home</a></li>
                        <li ><a href="index.html#section2">Experience</a></li> 
                        <li ><a href="index.html#section3">Education</a></li>
                        <li ><a href="index.html#section4">Projects</a></li>
                        <li ><a href="blog.html">Blog</a></li>
                        <li ><a href="index.html#section5">Contact Me</a></li>
                    </ul>
                </nav>
            </div>
            <div class="userbox2">
                <div class="container">
                    <h2 class="blog-title">Paper Review: Deep neuroethology of a Virtual Rodent</h2>
                    <h5 class="blog-title">Parallel developments in neuroscience and deep learning ?</h5>
                    <small class="text-muted citation">Merel, J., Aldarondo, D., Marshall, J., Tassa, Y., Wayne, G., & Ölveczky, B. (2019). Deep neuroethology of a virtual rodent. arXiv preprint arXiv:1911.09451.</small><br/>
                    <div class="row justify-content-center blog-content">
                        <div class="col-10">
                            <h5>Introduction</h5>
                            <p>This is an interesting paper that uses the algorithms of deep reinforcement learning and computational neuroscience to design a virtual rodent that performs some complex tasks in physical environment setting. The model is equipped with actuators, sensors and proprioceptive inputs, controlled by neural network to perform coordinated movements. This approach certainly provides a broad analysis of how an animal uses its senses and body to execute these tasks. The author managed to impressively use the data analysis techniques, enabling the agent to conduct flexible cognitive movements with egocentric stimuli.</p>

                            <h5>Model Architecture and Implementation</h5>
                            <p>The virtual rodent’s body is designed based on measurements of actual rats. The body has 38 controllable degrees of freedom and a RGB camera for proprioceptive information which is mounted on its head. It receives proprioceptive inputs regarding joint angles, velocity and positions of tendons and egocentric vectors from touch sensor. The model design is shown in the images below:</p>
                            <img alt="" src="./img/rodent.png" class="img-fluid blog-image"/>
                            <small class="text-muted citation">Figure 1: (A) Anatomical skeleton of a rodent (as reference; not part of physical simulation). (B) A body designed around the skeleton to match the anatomy and model collisions with the environment.
                            (C) Purely cosmetic skin to cover the body. (D) Semi-transparent visualization of (A)-(C) overlain.</small><br><br>

                            <p>The researchers implemented four tasks for the rodent and trained the neural network to guide the rodent through these tasks. These tasks are an implementation of reinforcement learning and motor neuroscience and the tasks are as follows:</p>

                            <ul class="bullet-list">
                                <li>Run along a corridor, over “gaps”, with a reward for traveling along the corridor at a target velocity.</li>
                                <li>Collect all the blue orbs in a maze, with a sparse reward for each orb collected.</li>
                                <li>Escape a bowl-shaped region by traversing hilly terrain, with a reward proportional to distance from the center of the bowl</li>
                                <li>Approach orbs in an open field and activate them by tapping two times with their forepaw, with precise interval of 800ms and tolerance of 100ms.</li>
                            </ul>
                            <img alt="" src="./img/vr_diag.png" class="img-fluid blog-image"/>
                            <p>The architecture of the model is based on backpropagation by training core module on multiple motor control task. First the egocentric visual image inputs are encoded into features and the proprioceptive state observations are encoded through a multi-layer perceptron. The outputs along with encoded features and proprioceptive observations are passed to the policy module. The policy module consists of stacked LSTM which produces actions. This method produces a single neural network that uses visual inputs to determine how to coordinates its body movements in order to solve the tasks.</p>


                            <h5>Analysis</h5>
                            <p>The analysis performed by the author was well articulated through this paper which provided surplus information about the behavioral features and task levels incorporated in the research. We found that the cell activity of core and policy layers had greater similarity with behavioral and postural features. And the policy activity is more similar across tasks than is core activity, which is very different across tasks. We also learnt that the different behaviors were seemingly associated with distinct rotations in neural activity space which follow stable orbits evolved at different timescales.</p>
                            

                            <h5>Concluding Thoughts</h5>
                            <p>I found the author’s attempt fascinating as they tried to incorporate the knowledge of two different fields- deep neural net and animal research and trained and analyzed the model using various dimensionality reduction and visualization methods. The description and visualization of the training provided to the model is detailed and the analysis shown through the video is quite illustrative as well. At the same time, I also agree that similar research showing implementation of neuroethology and deep learning have also been presented previously with similar findings. But I believe that the learning achieved through this paper will motivate many new studies and underpin new research in this field.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
</body>
</html>