<!DOCTYPE html>
<html>
<head>
    <title>Pooja Gaurav</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="blog.css">
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body>
    <section id="section1">
        <div class="content-box">
            <div class="userbox1"> 
                <nav class="fixed">
                    <center class="dp">
                        <img src="./img/dp1.jpg" class="home-img"/>
                        <h2>Pooja Gaurav</h2>
                    </center> 
                    <ul class="menu">
                        <li class = "item"><a href="index.html#section1">Home</a></li>
                        <li ><a href="index.html#section2">Experience</a></li> 
                        <li ><a href="index.html#section3">Education</a></li>
                        <li ><a href="index.html#section4">Projects</a></li>
                        <li ><a href="blog.html">Blog</a></li>
                        <li ><a href="index.html#section5">Contact Me</a></li>
                    </ul>
                </nav>
            </div>
            <div class="userbox2">
                <div class="container">
                    <h2 class="blog-title">Paper Review: World Model</h2>
                    <small class="text-muted citation">Ha, D., & Schmidhuber, J. (2018). World models. arXiv preprint arXiv:1803.10122.</small><br/>
                    
                    <div class="row justify-content-center blog-content">
                        <div class="col-10">
                            <h5>Introduction</h5>
                            <p>This paper presents a different approach to sequence-to-sequence modelling, by introducing a new model which uses multi head attention mechanism -Transformer.  The model uses encoder and decoder structure containing feed forward network, which can access information from whole sequence, from all past encoders and decoders. It is the different from previous models that relied on convolution or RNN for input output representation.</p>

                            <h5>Transformer Architecture</h5>
                            <p>The transformer uses 6 stacked self-attention layers of encoder and decoder. There are 6 encoders and decoders. The attention functions maps queries with all keys and apply SoftMax function to obtain weights on values parallelly. And along with the sublayers, each layer contains a feed forward network.</p>
                            <p style="text-align:center;">
                            <img alt="" src="./img/attn.png" class="img-fluid blog-image center" height="500" width="300"/></p>

                            <p>The model is trained on WMT 2014 English â€“ German dataset consisting of 4.5 million sentences pairs and English - French dataset with 36 million sentence pair. It took 100,000 steps or 12 hours to train the base model and 3.5 days to train big model on 8 NVIDIA P100 GPUs. </p>

                            <p style="text-align:center;">
                            <img alt="" src="./img/ppr11.png" class="img-fluid blog-image center" height="400" width="300"/></p>
                          

                            <h6>Strong Points</h6>
                            <p>The paper presents an entirely new approach without using recurrent or convolution method and give a detailed outline of the model to be used in future research. And the model performance is incredible, outperforming the previous models on two language pairs in WMT 2014 translation tasks.
                            </p>

                            <h6>Weak Points</h6>
                            <p>Though the paper presents many possibilities for attention mechanism for implementation in future, but the model uses many hypermeters which increases the time it takes to train. So, if we need to train it using huge corpus of data, a lot of our experiments time will go in training the model. 
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
</body>
</html>